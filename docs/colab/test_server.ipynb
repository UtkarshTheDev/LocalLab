{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LocalLab Server Test Notebook\n",
        "\n",
        "This notebook helps test the LocalLab server package on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, let's make sure we have a clean environment\n",
        "!pip uninstall -y locallab\n",
        "!pip cache purge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required dependencies first\n",
        "!pip install --upgrade transformers accelerate\n",
        "!pip install torch pyngrok fastapi uvicorn huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install the package from TestPyPI\n",
        "!pip install locallab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set environment variables\n",
        "import os\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Set your ngrok token\n",
        "NGROK_TOKEN = input(\"Enter your ngrok token: \").strip()\n",
        "if not NGROK_TOKEN:\n",
        "    raise ValueError(\"Ngrok token is required for running the server\")\n",
        "os.environ[\"NGROK_AUTH_TOKEN\"] = NGROK_TOKEN\n",
        "\n",
        "# Choose model configuration\n",
        "print(\"\\nAvailable default models:\")\n",
        "print(\"1. microsoft/phi-2 (Default, 2.7B parameters)\")\n",
        "print(\"2. TinyLlama/TinyLlama-1.1B-Chat-v1.0 (1.1B parameters)\")\n",
        "print(\"3. stabilityai/stable-code-3b (3B parameters)\")\n",
        "print(\"4. Custom model from Hugging Face\")\n",
        "\n",
        "choice = input(\"\\nChoose model (1-4, default is 1): \").strip() or \"1\"\n",
        "\n",
        "# Configure model using CLI\n",
        "if choice == \"4\":\n",
        "    custom_model = input(\"Enter Hugging Face model ID (e.g., meta-llama/Llama-3.2-3B-Instruct): \").strip()\n",
        "    if not custom_model:\n",
        "        raise ValueError(\"Custom model ID is required when choosing option 4\")\n",
        "    !locallab config --model {custom_model}\n",
        "else:\n",
        "    models = {\n",
        "        \"1\": \"microsoft/phi-2\",\n",
        "        \"2\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "        \"3\": \"stabilityai/stable-code-3b\"\n",
        "    }\n",
        "    model_name = models.get(choice, \"microsoft/phi-2\")\n",
        "    !locallab config --model {model_name}\n",
        "\n",
        "# Configure performance settings using CLI\n",
        "!locallab config --flash-attention true --attention-slicing true --cpu-offload true \\\n",
        "                 --better-transformer true --quantize true --quantize-type int8 \\\n",
        "                 --min-free-memory 2000\n",
        "\n",
        "logging.info(f\"Configuration complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import and check if locallab is installed correctly\n",
        "try:\n",
        "    import locallab\n",
        "    print(\"Successfully imported locallab\")\n",
        "except ImportError as e:\n",
        "    print(f\"Import error: {e}\")\n",
        "    print(\"\\nTrying to find the module:\")\n",
        "    !find /usr/local/lib/python3.* -name \"locallab*\"\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start the server using CLI (notice the ! prefix)\n",
        "try:\n",
        "    # Start with the configured model and settings\n",
        "    !locallab start --use-ngrok\n",
        "except Exception as e:\n",
        "    print(f\"Error starting server: {str(e)}\")\n",
        "    print(\"\\nTrying to fall back to default model...\")\n",
        "    try:\n",
        "        # Try with a smaller model and optimizations\n",
        "        !locallab config --model phi-2 --quantize true --quantize-type int8\n",
        "        !locallab start --use-ngrok\n",
        "    except Exception as e2:\n",
        "        print(f\"Fallback also failed: {str(e2)}\")\n",
        "        print(\"\\nPlease try the following:\")\n",
        "        print(\"1. Restart the runtime\")\n",
        "        print(\"2. Check your internet connection\")\n",
        "        print(\"3. Verify your ngrok token\")\n",
        "        print(\"4. Try a smaller model\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LocalLab Server Test",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
