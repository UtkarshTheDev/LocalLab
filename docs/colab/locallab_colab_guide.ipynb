{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸš€ LocalLab on Google Colab\n",
        "\n",
        "This notebook provides a complete guide to running LocalLab on Google Colab. Follow each section step by step.\n",
        "\n",
        "## Table of Contents\n",
        "1. Setup & Installation\n",
        "2. Configuration\n",
        "3. Model Loading\n",
        "4. Usage Examples\n",
        "5. Monitoring & Optimization\n",
        "6. Troubleshooting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Installation\n",
        "\n",
        "First, let's install LocalLab and its dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "!pip install locallab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "\n",
        "### 2.1 Enter Your Tokens\n",
        "Please enter your ngrok and Hugging Face tokens below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "print(\"Enter your ngrok token (get one from https://dashboard.ngrok.com):\")\n",
        "ngrok_token = getpass()\n",
        "os.environ[\"NGROK_AUTH_TOKEN\"] = ngrok_token\n",
        "\n",
        "print(\"\\nEnter your Hugging Face token (optional, get from https://huggingface.co/settings/tokens):\")\n",
        "hf_token = getpass()\n",
        "if hf_token:\n",
        "    os.environ[\"HUGGINGFACE_TOKEN\"] = hf_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Select Model\n",
        "Choose your model configuration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# @title Model Configuration\n",
        "model_choice = \"microsoft/phi-2\" # @param [\"microsoft/phi-2\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"custom\"]\n",
        "custom_model = \"\" # @param {type:\"string\"}\n",
        "\n",
        "# Method 1: Using CLI (recommended)\n",
        "if model_choice == \"custom\" and custom_model:\n",
        "    !locallab config --model {custom_model}\n",
        "else:\n",
        "    !locallab config --model {model_choice}\n",
        "\n",
        "# Method 2: Using environment variables (backup method)\n",
        "if model_choice == \"custom\":\n",
        "    os.environ[\"HUGGINGFACE_MODEL\"] = custom_model\n",
        "else:\n",
        "    os.environ[\"HUGGINGFACE_MODEL\"] = model_choice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Performance Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# @title Optimization Settings\n",
        "enable_quantization = True # @param {type:\"boolean\"}\n",
        "quantization_type = \"int8\" # @param [\"fp16\", \"int8\", \"int4\"]\n",
        "enable_flash_attention = True # @param {type:\"boolean\"}\n",
        "enable_attention_slicing = True # @param {type:\"boolean\"}\n",
        "\n",
        "# Method 1: Using CLI (recommended)\n",
        "!locallab config --quantize {str(enable_quantization).lower()} --quantize-type {quantization_type} \\\n",
        "                 --flash-attention {str(enable_flash_attention).lower()} \\\n",
        "                 --attention-slicing {str(enable_attention_slicing).lower()}\n",
        "\n",
        "# Method 2: Using environment variables (backup method)\n",
        "os.environ[\"LOCALLAB_ENABLE_QUANTIZATION\"] = str(enable_quantization)\n",
        "os.environ[\"LOCALLAB_QUANTIZATION_TYPE\"] = quantization_type\n",
        "os.environ[\"LOCALLAB_ENABLE_FLASH_ATTENTION\"] = str(enable_flash_attention)\n",
        "os.environ[\"LOCALLAB_ENABLE_ATTENTION_SLICING\"] = str(enable_attention_slicing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Start Server\n",
        "\n",
        "Now let's start the LocalLab server:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Start server with ngrok enabled using CLI (notice the ! prefix)\n",
        "!locallab start --use-ngrok\n",
        "\n",
        "# This will show the public URL in logs like:\n",
        "# ðŸš€ Ngrok Public URL: https://abc123.ngrok.app\n",
        "# COPY THIS URL - you'll need it to connect!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Connect Client\n",
        "\n",
        "Copy the ngrok URL from the logs above and use it to connect the client:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "from locallab_client import LocalLabClient\n",
        "\n",
        "# @title Client Connection\n",
        "server_url = \"\" # @param {type:\"string\"}\n",
        "\n",
        "client = LocalLabClient(server_url)\n",
        "\n",
        "# Test connection\n",
        "is_healthy = await client.health_check()\n",
        "print(f\"Server connection: {'OK' if is_healthy else 'Failed'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Usage Examples\n",
        "\n",
        "### 5.1 Basic Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# @title Text Generation\n",
        "prompt = \"Write a story about a robot learning to paint\" # @param {type:\"string\"}\n",
        "temperature = 0.7 # @param {type:\"slider\", min:0.1, max:1.0, step:0.1}\n",
        "\n",
        "response = await client.generate(prompt, temperature=temperature)\n",
        "print(\"AI Response:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Chat Completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# @title Chat with AI\n",
        "system_message = \"You are a helpful assistant.\" # @param {type:\"string\"}\n",
        "user_message = \"What is artificial intelligence?\" # @param {type:\"string\"}\n",
        "\n",
        "response = await client.chat([\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_message}\n",
        "])\n",
        "\n",
        "print(\"AI Response:\")\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Streaming Response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# @title Stream Generation\n",
        "stream_prompt = \"Tell me a story\" # @param {type:\"string\"}\n",
        "\n",
        "print(\"AI Response:\")\n",
        "async for token in client.stream_generate(stream_prompt):\n",
        "    print(token, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. System Monitoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Get system information\n",
        "system_info = await client.get_system_info()\n",
        "\n",
        "print(\"System Status:\")\n",
        "print(f\"CPU Usage: {system_info.cpu_usage}%\")\n",
        "print(f\"Memory Usage: {system_info.memory_usage}%\")\n",
        "if system_info.gpu_info:\n",
        "    print(f\"GPU: {system_info.gpu_info.device}\")\n",
        "    print(f\"GPU Memory Used: {system_info.gpu_info.used_memory}MB\")\n",
        "print(f\"Active Model: {system_info.active_model}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Cleanup\n",
        "\n",
        "When you're done, clean up resources:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Unload model to free memory\n",
        "await client.unload_model()\n",
        "\n",
        "# Close client connection\n",
        "await client.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Troubleshooting\n",
        "\n",
        "If you encounter issues:\n",
        "\n",
        "1. **Connection Issues**\n",
        "   - Verify ngrok token is correct\n",
        "   - Check server logs for errors\n",
        "   - Ensure URL is copied correctly\n",
        "\n",
        "2. **Memory Issues**\n",
        "   - Enable quantization\n",
        "   - Try a smaller model\n",
        "   - Check GPU memory usage\n",
        "\n",
        "3. **Model Loading Issues**\n",
        "   - Verify Hugging Face token\n",
        "   - Check model name is correct\n",
        "   - Ensure sufficient resources\n",
        "\n",
        "For more help, visit:\n",
        "- [Troubleshooting Guide](https://github.com/UtkarshTheDev/LocalLab/blob/main/docs/TROUBLESHOOTING.md)\n",
        "- [FAQ](https://github.com/UtkarshTheDev/LocalLab/blob/main/docs/FAQ.md)\n",
        "- [GitHub Issues](https://github.com/UtkarshTheDev/LocalLab/issues)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LocalLab_Guide.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
